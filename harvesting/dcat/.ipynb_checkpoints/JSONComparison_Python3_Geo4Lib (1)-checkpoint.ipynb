{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Original created on Wed Mar 15 09:18:12 2017\n",
    "Edited Dec 28 2018; January 8, 2019; Dec 26-31, 2019\n",
    "@author: kerni016\n",
    "\"\"\"\n",
    "## To run this script you need a csv with six columns (portalName, URL, provenance, isPartOf, publisher, and spatialCoverage) with details about ESRI open data portals to be checked for new records.\n",
    "## Need to define PreviousActionDate and ActionDate, directory path (containing newAll.csv and folders \"Jsons\" and \"Reports\"), and list of fields desired in the printed report\n",
    "## The script currently prints two combined reports - one of new items and one with deleted items.  Commented code allows the option to also print reports for each data portal.\n",
    "\n",
    "## Remaining Quesitons / Possible Improvements:\n",
    "## if new, check to see whether record similar to something that was there before?\n",
    "\n",
    "import json\n",
    "import csv\n",
    "import urllib.request\n",
    "import os\n",
    "import os.path\n",
    "from html.parser import HTMLParser\n",
    "import decimal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Manual items to change!\n",
    "\n",
    "## Set the date download of the older and newer jsons\n",
    "ActionDate = 'newtest'\n",
    "PreviousActionDate = '20190104'\n",
    "\n",
    "## names of the main directory containing folders named \"Jsons\" and \"Reports\"\n",
    "directory = '/Users/majew030/GitHUB/dcat-metadata/'\n",
    "\n",
    "##list of metadata fields from the DCAT json schema for open data portals desired in the final report\n",
    "fieldnames = [\"identifier\", \"code\", \"title\", \"alternativeTitle\", \"description\", \"genre\", \"subject\", \"format\", \"type\", \"geometryType\", \"dateIssued\", \"temporalCoverage\", \"Date\", \"spatialCoverage\", \"spatial\", \"provenance\", \"publisher\",  \"creator\", \"landingPage\", \"downloadURL\", \"webService\", \"metadataURL\", \"serverType\", \"keywords\"]\n",
    "\n",
    "##list of fields to use for the deletedItems report\n",
    "delFieldsReport = ['identifier', 'landingPage', 'portalName']\n",
    "\n",
    "##list of fields to use for the portal status report\n",
    "statusFieldsReport = ['portalName', 'total', 'new_items', 'deleted_items']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Removes html tags from text and replaces non-ascii characters with \"?\"\n",
    "###Code derived from Eloff on Stack Overflow : https://stackoverflow.com/questions/753052/strip-html-from-strings-in-python\n",
    "\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs= True\n",
    "        self.fed = []\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "    def get_data(self):\n",
    "        return ''.join(self.fed)\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()\n",
    "\n",
    "def cleanData (value):\n",
    "    fieldvalue = strip_tags(value)\n",
    "    fieldvalue = fieldvalue.encode('ascii', 'replace').decode('utf-8')\n",
    "    return fieldvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### function that prints metadata elements from a dictionary to a csv file with as specified fields list as the header row\n",
    "def printReport (report, dictionary, fields):\n",
    "    with open(report, 'w', newline='') as outfile:\n",
    "        csvout = csv.writer(outfile)\n",
    "        csvout.writerow(fields)\n",
    "        for keys in dictionary:\n",
    "            allvalues = dictionary[keys]\n",
    "            csvout.writerow(allvalues)\n",
    "\n",
    "### function that creates a dictionary with the position of a record in the data portal DCAT metadata json as the key and the identifier as the value\n",
    "def getIdentifiers (data):\n",
    "    json_ids = {}\n",
    "    for x in range(len(data[\"dataset\"])):\n",
    "        json_ids[x] = data[\"dataset\"][x][\"identifier\"]\n",
    "    return json_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###function that returns a dictionary of selected metadata elements for each new item in a data portal. This includes blank fields '' for columns that will be filled in manually later. Input requires the current DCAT json of the a data portal and a dictionary with key (position in the DCAT json), value (landing page URL) of the items. Includes an option to print a csv report of new items for each data portal\n",
    "def metadataNewItems(newdata, newitem_ids):\n",
    "    newItemDict = {}\n",
    "    ### key = position of the dataset in the DCAT metadata json, value = landing page URLs \n",
    "    for key, value in newitem_ids.items():\n",
    "        ###each time through the loop, creates an empty list to hold metadata about each item\n",
    "        metadata = []\n",
    "        \n",
    "        identifier = value\n",
    "        metadata.append(identifier.rsplit('/', 1)[-1])\n",
    "        metadata.append(portalName)\n",
    "        try:\n",
    "            metadata.append(cleanData(newdata[\"dataset\"][key]['title']))  #*\n",
    "        except:\n",
    "            metadata.append(newdata[\"dataset\"][key]['title'])\n",
    "\n",
    "        altTitle = \"\"\n",
    "        metadata.append(altTitle)\n",
    "        metadata.append(cleanData(newdata[\"dataset\"][key]['description'])) #*\n",
    "\n",
    "        ### Set default blank values for genre, format, type, and downloadURL\n",
    "        format_types = []\n",
    "        genre = \"\"\n",
    "        formatElement = \"\"\n",
    "        typeElement = \"\"\n",
    "        downloadURL =  \"\"\n",
    "        geometryType = \"\"\n",
    "        webService = \"\"\n",
    "\n",
    "        distribution = newdata[\"dataset\"][key][\"distribution\"]\n",
    "        for dictionary in distribution:\n",
    "            try:\n",
    "                ### If one of the distributions is a shapefile, change genre/format and get the downloadURL\n",
    "                format_types.append(dictionary[\"title\"])\n",
    "                if dictionary[\"title\"] == \"Shapefile\":\n",
    "                    genre = \"Geospatial data\"\n",
    "                    formatElement = \"Shapefile\"\n",
    "                    if 'downloadURL' in dictionary.keys():\n",
    "                        downloadURL = dictionary[\"downloadURL\"]\n",
    "                    else:\n",
    "                        downloadURL = dictionary[\"accessURL\"]\n",
    "\n",
    "                    geometryType = \"Vector\"\n",
    "\n",
    "                ### If the Rest API is based on an ImageServer, change genre, type, and format to relate to imagery\n",
    "                if dictionary[\"title\"] == \"Esri Rest API\":\n",
    "                    #imageCheck = dictionary['accessURL'].rsplit('/', 1)[-1]\n",
    "                    if 'accessURL' in dictionary.keys():\n",
    "                        webService = dictionary['accessURL']\n",
    "                    \n",
    "                        if webService.rsplit('/', 1)[-1] == 'ImageServer':\n",
    "                            genre = \"Aerial imagery\"\n",
    "                            formatElement = 'Imagery'\n",
    "                            typeElement = 'Image|Service'\n",
    "                            #### Change this to Raster or Image?\n",
    "                            geometryType = \"\"\n",
    "\n",
    "                        ### If one of the distributions is a pdf, change genre and format\n",
    "                        elif \".pdf\" in webService:\n",
    "                            genre = 'Flagged'\n",
    "                            formatElement = 'PDF'\n",
    "                            typeElement = \"\"\n",
    "                            downloadURL =  \"\"\n",
    "\n",
    "                        ### If one of the distributions is a web application, change genre and format\n",
    "                        elif \"/apps/\" in webService:\n",
    "                            genre = 'Flagged'\n",
    "                            formatElement = 'Web application'\n",
    "                            typeElement = \"\"\n",
    "                            downloadURL =  \"\"\n",
    "                    else:\n",
    "                        genre = \"error - no download URL\"\n",
    "                        formatElement = \"error\"\n",
    "                        typeElement = \"\"\n",
    "                        downloadURL =  \"\"\n",
    "\n",
    "            ### If the distribution section of the metadata is not structured in a typical way\n",
    "            except:\n",
    "                ### Set default error values for genre, format, type, and downloadURL\n",
    "                genre = \"error - dictionary structure\"\n",
    "                formatElement = \"error\"\n",
    "                typeElement = \"error\"\n",
    "                downloadURL =  \"error\"\n",
    "\n",
    "                continue            \n",
    "          \n",
    "                \n",
    "        ###If the item has both a Shapefile and Esri Rest API format, change type\n",
    "        if \"Esri Rest API\" in format_types:\n",
    "            if \"Shapefile\" in format_types:\n",
    "                typeElement = \"Dataset|Service\"\n",
    "        ### If the distribution section is well structured but doesn't include either a shapefile or imagery, add a list of format types and set genre to 'flagged\n",
    "        if formatElement == \"\":\n",
    "            genre = 'flagged - other format'\n",
    "            formatElement = '|'.join(format_types)\n",
    "\n",
    "        ### Checks for patterns in spatial coordinates that frequently indicate an error and, if found, changes the genre to \"Suspicious coordinates\"\n",
    "\n",
    "        try:\n",
    "            bbox = []\n",
    "            spatial = cleanData(newdata[\"dataset\"][key]['spatial']) #*\n",
    "            typeDmal = decimal.Decimal\n",
    "            fix4 = typeDmal(\"0.0001\")\n",
    "            for coord in spatial.split(\",\"):\n",
    "                coordFix = typeDmal(coord).quantize(fix4)\n",
    "                bbox.append(str(coordFix))\n",
    "            count = 0\n",
    "            for coord in bbox:\n",
    "                if coord == '0.0000':\n",
    "                    count += 1\n",
    "            if count >= 2:\n",
    "                genre = 'flagged - suspicious coordinates'\n",
    "        except:\n",
    "            spatial = \"\"\n",
    "\n",
    "\n",
    "        metadata.append(genre)\n",
    "        subject = \"\"\n",
    "        metadata.append(subject)\n",
    "        metadata.append(formatElement)\n",
    "        metadata.append(typeElement)\n",
    "        metadata.append(geometryType)\n",
    "\n",
    "        metadata.append(cleanData(newdata[\"dataset\"][key]['issued'])) #*\n",
    "        temporalCoverage = \"\"\n",
    "        metadata.append (temporalCoverage)\n",
    "        dateElement = \"\"\n",
    "        metadata.append(dateElement)\n",
    "        metadata.append(spatialCoverage)\n",
    "        metadata.append(spatial)\n",
    "\n",
    "        metadata.append(provenance)\n",
    "#       metadata.append(isPartOf)\n",
    "        metadata.append(publisher)\n",
    "        \n",
    "        creator = newdata[\"dataset\"][key][\"publisher\"]\n",
    "        for pub in creator.values():\n",
    "            creator = pub.encode('ascii', 'replace').decode('utf-8')\n",
    "#       creator = creator['source'].encode('ascii', 'replace')\n",
    "        metadata.append(creator)\n",
    "\n",
    "        metadata.append(cleanData(newdata[\"dataset\"][key]['landingPage'])) #*\n",
    "        metadata.append(downloadURL)\n",
    "        metadata.append(webService)\n",
    "#       metadata.append(cleanData(newdata[\"dataset\"][key]['webService']))  #*\n",
    "        metadataLink = \"\"\n",
    "        metadata.append(metadataLink)\n",
    "\n",
    "#       webService = cleanData(newdata[\"dataset\"][key]['webService'])  #*\n",
    "\n",
    "        serviceType = \"\"\n",
    "        serviceTypeList = [\"FeatureServer\", \"MapServer\", \"ImageServer\"]\n",
    "        for server in serviceTypeList:\n",
    "            try:\n",
    "                if server in webService:\n",
    "                    serviceType = server\n",
    "            except:\n",
    "                print(identifier)\n",
    "        metadata.append(serviceType)\n",
    "\n",
    "        keywords = newdata[\"dataset\"][key][\"keyword\"]\n",
    "        keyword_list = '|'.join(keywords)\n",
    "        metadata.append(keyword_list)\n",
    "\n",
    "        newItemDict[identifier] = metadata\n",
    "    ###Uncomment to print reports for individual portals\n",
    "#    if len(newItemDict) > 0:\n",
    "#        reportNew = directory + \"\\Reports\\%s_%s_new_itemsReport.csv\" % (portalName, ActionDate)\n",
    "#        printReport(reportNew, newItemDict, fieldnames)\n",
    "#        print \"new item report complete for %s!\" % (portalName)\n",
    "    return newItemDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Sets up lists to hold metadata information from each portal to be printed to a report\n",
    "All_New_Items = []\n",
    "All_Deleted_Items = []\n",
    "Status_Report = {}\n",
    "\n",
    "### Opens a list of portals and urls ending in /data.json from input CSV using column headers 'portalName' and 'URL'\n",
    "with open(directory + 'arcPortals.csv') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        ### Read in values from the portals list to be used within the script or as part of the metadata report\n",
    "        portalName = row['portalName']\n",
    "        url = row['URL']\n",
    "        provenance = row['provenance']\n",
    "#         isPartOf = row['isPartOf']\n",
    "        publisher = row['publisher']\n",
    "        spatialCoverage = row['spatialCoverage']\n",
    "        print (portalName, url)\n",
    "\n",
    "        ## for each open data portal in the csv list...\n",
    "        ## renames file paths based on portalName and manually provided dates\n",
    "        oldjson = directory + 'Jsons/%s_%s.json' % (portalName, PreviousActionDate)\n",
    "        newjson = directory + 'Jsons/%s_%s.json' % (portalName, ActionDate)\n",
    "\n",
    "        try:\n",
    "            response = urllib.request.urlopen(url)\n",
    "            newdata = json.load(response)\n",
    "        \n",
    "        except:\n",
    "            print (\"Data portal URL does not exist: \" + url)\n",
    "            break\n",
    "\n",
    "            \n",
    "        ### Saves a copy of the json to be used for the next round of comparison/reporting\n",
    "        with open(newjson, 'w') as outfile:\n",
    "            json.dump(newdata, outfile)\n",
    "\n",
    "            ### Prints a warning if there are more than 1000 resources in the data portal - no longer needed as of 12/2019 revisions\n",
    "            #if len(newdata[\"dataset\"]) == 999:\n",
    "                #print (\"Warning! More than 1000 data resources in %s!\" % (portalName))\n",
    "\n",
    "            ### collects information about number of resources (total, new, and old) in each portal\n",
    "            status_metadata = []\n",
    "            status_metadata.append(portalName)\n",
    "\n",
    "        #Opens older copy of data/json downloaded from the specified Esri Open Data Portal.  If this file does not exist, treats every item in the portal as new\n",
    "        if os.path.exists(oldjson):\n",
    "            with open(oldjson) as data_file:\n",
    "                older_data = json.load(data_file)\n",
    "\n",
    "            ### Makes a list of dataset identifiers in the older json\n",
    "            older_ids = getIdentifiers (older_data)\n",
    "\n",
    "            ###compares identifiers in the older json harvest of the data portal with identifiers in the new json, creating dictionaries with 1) a complete list of new json identifiers and 2) a list of just the items that appear in the new json but not the older one\n",
    "            newjson_ids = {}\n",
    "            newitem_ids = {}\n",
    "\n",
    "            for y in range(len(newdata[\"dataset\"])):\n",
    "                identifier = newdata[\"dataset\"][y][\"identifier\"]\n",
    "                newjson_ids[y] = identifier\n",
    "                if identifier not in older_ids.values():\n",
    "                    newitem_ids[y] = identifier\n",
    "\n",
    "\n",
    "            ### creates a dictionary of metadata elements for each new data portal item. Includes an option to print a csv report of new items for each data portal\n",
    "            ### Puts dictionary of identifiers (key), metadata elements (values) for each data portal into a list (to be used printing the combined report) [portal1{identifier:[metadataElement1, metadataElement2, ... ], portal2{identifier:[metadataElement1, metadataElement2, ... ], ...}\n",
    "            All_New_Items.append(metadataNewItems(newdata, newitem_ids))\n",
    "\n",
    "            ### collects information for the status report about the number of records currently in the portal and new items\n",
    "            status_metadata.append(len(newjson_ids))\n",
    "            status_metadata.append(len(newitem_ids))\n",
    "\n",
    "            ### Compares identifiers in the older json to the list of identifiers from the newer json. If the record no longer exists, adds selected fields into a dictionary of deleted items (deletedItemDict)\n",
    "            deletedItemDict = {}\n",
    "            for z in range(len(older_data[\"dataset\"])):\n",
    "                identifier = older_data[\"dataset\"][z][\"identifier\"]\n",
    "                if identifier not in newjson_ids.values():\n",
    "                    del_metadata = []\n",
    "                    del_metadata.append(identifier.rsplit('/', 1)[-1])\n",
    "                    del_metadata.append(identifier)\n",
    "                    del_metadata.append(portalName)\n",
    "                    deletedItemDict[identifier] = del_metadata\n",
    "\n",
    "            ### Puts dictionary of identifiers (key), metadata elements (values) for each data portal into a list (to be used printing the combined report) [portal1{identifier:[metadataElement1, metadataElement2, ... ], portal2{identifier:[metadataElement1, metadataElement2, ... ], ...}\n",
    "            All_Deleted_Items.append(deletedItemDict)\n",
    "            ###Uncomment to print reports for individual portals\n",
    "#            if len(deletedItemDict) > 0:\n",
    "#                reportDelete = directory + \"\\Reports\\%s_%s_deleted_itemsReport.csv\" % (portalName, ActionDate)\n",
    "#                printReport(reportDelete, deletedItemDict, delFieldsReport)\n",
    "#                print \"deleted items report complete for %s!\" % (portalName)\n",
    "\n",
    "            ### collects information for the status report about the number of deleted items\n",
    "            status_metadata.append(len(deletedItemDict))\n",
    "            Status_Report [portalName] = status_metadata\n",
    "\n",
    "        ### if there is no older json for comparions....\n",
    "        else:\n",
    "            print (\"There is no comparison json for %s\" % (portalName))\n",
    "            ### Makes a list of dataset identifiers in the new json\n",
    "            newjson_ids = getIdentifiers (newdata)\n",
    "\n",
    "            ### creates a dictionary of metadata elements for each new item in a data portal (i.e. all items from the new json). Includes an option to print a csv report of new items for each data portal\n",
    "            ### Puts dictionary of identifiers (key), metadata elements (values) for each data portal into a list (to be used printing the combined report)   [portal1{identifier:[metadataElement1, metadataElement2, ... ], portal2{identifier:[metadataElement1, metadataElement2, ... ], ...}\n",
    "            All_New_Items.append(metadataNewItems(newdata, newjson_ids))\n",
    "\n",
    "            ### collects information for the status report about the number of records currently in the portal, new items, and deleted items\n",
    "            status_metadata.append(len(newjson_ids))\n",
    "            status_metadata.append(len(newjson_ids))\n",
    "            status_metadata.append('0')\n",
    "            Status_Report [portalName] = status_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### prints two csv spreadsheets with all items that are new or deleted since the last time the data portals were harvested\n",
    "report = directory + \"allNewItems_%s.csv\" %  (ActionDate)\n",
    "with open(report, 'w', newline='', encoding=\"utf-8\") as outfile:\n",
    "        csvout = csv.writer(outfile)\n",
    "        csvout.writerow(fieldnames)\n",
    "        for portal in All_New_Items:\n",
    "            for keys in portal:\n",
    "                allvalues = portal[keys]\n",
    "                csvout.writerow(allvalues)\n",
    "\n",
    "report = directory + \"allDeletedItems_%s.csv\" %  (ActionDate)\n",
    "with open(report, 'w', newline='', encoding=\"utf-8\") as outfile:\n",
    "        csvout = csv.writer(outfile)\n",
    "        csvout.writerow(delFieldsReport)\n",
    "        for portal in All_Deleted_Items:\n",
    "            for keys in portal:\n",
    "                allvalues = portal[keys]\n",
    "                csvout.writerow(allvalues)\n",
    "\n",
    "reportStatus = directory + \"Reports/portal_status_report_%s.csv\" % (ActionDate)\n",
    "printReport (reportStatus, Status_Report, statusFieldsReport)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
